% Use "url" para colocar links e "urlaccessdate" para colocar as datas %
%  Ex: url = {https://github.com/davimedio01}                     
%   urlaccessdate = {03 jan. 2023.}

% Outras Solucoes

@misc{agenda-socialbauru,
    title = {Agenda Social Bauru},
    url = {https://www.socialbauru.com.br/agenda/},
        urlaccessdate = {3 set. 2024},
    language = {pt},
    urldate = {2024-09-03},
    author = {{SOCIAL BAURU}},
    year = {2024},
}

@misc{agenda-jauclick,
    title = {Agenda JauClick},
    url = {https://jauclick.com/agenda/},
        urlaccessdate = {3 set. 2024},
    language = {pt},
    urldate = {2024-09-03},
    author = {{JAUCLICK}},
    year = {2024},
}

@misc{agenda-jcnet,
    title = {Agenda JC},
    url = {https://sampi.net.br/bauru/noticias/2794605/agenda/2023/10/agenda-jc-},
        urlaccessdate = {10 set. 2024},
    language = {pt},
    urldate = {2024-09-10},
    author = {{JORNAL DA CIDADE}},
    year = {2024},
}

@misc{plataforma-qualaboa,
    title = {Qualaboa},
    url = {https://www.qualaboa.io/},
        urlaccessdate = {10 nov. 2025},
    language = {pt},
    urldate = {2025-11-10},
    author = {{QUALABOA}},
    year = {2025},
}

% introducao

@misc{direitos-humanos,
    title = {Declaração Universal dos Direitos Humanos},
    url = {https://www.unicef.org/brazil/declaracao-universal-dos-direitos-humanos},
        urlaccessdate = {26 nov. 2024},
    language = {pt},
    urldate = {1948-12-10},
    author = {{ORGANIZAÇÃO DAS NAÇÕES UNIDAS}},
    year = {1948},
}

@misc{acesso-cultura,
    title = {A importância da universalização do acesso à cultura},
    url = {https://www.terra.com.br/noticias/a-importancia-da-universalizacao-do-acesso-a-cultura,340ed17be98f33505f5735c2c647baa6u785ibfb.html},
        urlaccessdate = {3 set. 2024},
    language = {pt},
    urldate = {2024-04-04},
    author = {Vinícius de Andrade},
    year = {2024},
}

% tecnologias

@misc{piramide-testes,
    title = {O que é a Pirâmide de Testes?},
    url = {https://blog.onedaytesting.com.br/piramide-de-teses/},
        urlaccessdate = {3 set. 2024},
    language = {pt},
    urldate = {2021-08-30},
    author = {{WORLD HEALTH ORGANIZATION}},
    year = {2023},
}

% urldate = {xxxx-xx-xx},
@misc{postgresql,
    title = {What is PostgreSQL?},
    url = {https://www.postgresql.org/about/},
        urlaccessdate = {17 nov. 2025},
    language = {en},
    author = {{THE POSTGRESQL GLOBAL DEVELOPMENT GROUP}},
    year = {2025},
}

% urldate = {xxxx-xx-xx},
@misc{java,
    title = {O que é tecnologia Java e por que preciso dela?},
    url = {https://www.java.com/pt-BR/download/help/whatis_java.html},
        urlaccessdate = {17 nov. 2025},
    language = {pt},
    author = {{ORACLE}},
    year = {2025},
}

% urldate = {xxxx-xx-xx},
@misc{springboot,
    title = {Spring Boot Overview},
    url = {https://spring.io/projects/spring-boot},
        urlaccessdate = {17 nov. 2025},
    language = {en},
    author = {{BROADCOM}},
    year = {2025},
}

% urldate = {xxxx-xx-xx},
@misc{angular,
    title = {What is Angular?},
    url = {https://v18.angular.dev/overview},
        urlaccessdate = {17 nov. 2025},
    language = {en},
    author = {{GOOGLE}},
    year = {2024},
}

@misc{typescript,
    title = {What is TypeScript?},
    url = {https://www.typescriptlang.org/},
        urlaccessdate = {17 nov. 2025},
    language = {en},
    author = {{MICROSOFT}},
    year = {2025},
}

@misc{jwt-rfc,
    title = {RFC7519: JSON Web Token (JWT)},
    url = {https://www.jwt.io/introduction},
        urlaccessdate = {17 nov. 2025},
    language = {en},
    author = {Michael B. Jones and John Bradley and Nat Sakimura},
    year = {2015},
}
% author = {Michael B. Jones, John Bradley, Nat Sakimura},


@misc{jwt,
    title = {Introduction to JSON Web Tokens},
    url = {https://www.jwt.io/introduction},
        urlaccessdate = {17 nov. 2025},
    language = {en},
    author = {{AUTH0}},
    year = {2025},
}

@misc{cliente-servidor,
    title = {Client-Server Architecture},
    url = {https://www.ebsco.com/research-starters/architecture/client-server-architecture},
        urlaccessdate = {17 nov. 2025},
    language = {en},
    author = {Angela Harmon},
    year = {2024},
}

@misc{http-rfc,
    title = {Hypertext Transfer Protocol -- HTTP/1.1},
    url = {https://datatracker.ietf.org/doc/html/rfc2616#section-1.4},
        urlaccessdate = {17 nov. 2025},
    language = {en},
    author = {Henrik Nielsen and Jeffrey Mogul and Larry M Masinter and Roy T. Fielding and Jim Gettys and Paul J. Leach and Tim Berners-Lee},
    year = {1999},
}

@misc{api,
    title = {What is an API? },
    url = {https://www.ibm.com/think/topics/api?},
        urlaccessdate = {17 nov. 2025},
    language = {en},
    author = {Michael Goodwin},
    year = {2025},
}

@misc{rest,
    title = {Representational State Transfer (REST)},
    url = {https://www.service-architecture.com/articles/web-services/representational-state-transfer-rest.html},
        urlaccessdate = {17 nov. 2025},
    language = {en},
    author = {Douglas K. Barry},
    year = {2025},
}

@misc{pwa,
    title = {Overview of Progressive Web Apps (PWAs)},
    url = {https://learn.microsoft.com/en-us/microsoft-edge/progressive-web-apps/},
        urlaccessdate = {17 nov. 2025},
    language = {en},
    urldate = {2025-01-10},
    author = {{MICROSOFT}},
    year = {2025},
}

@misc{ionic,
    title = {The mobile SDK for the Web},
    url = {https://ionicframework.com/},
        urlaccessdate = {17 nov. 2025},
    language = {en},
    author = {{IONIC}},
    year = {2025},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Introdução %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@misc{oms_surdos_2023,
    title = {Deafness and hearing loss},
    url = {https://www.who.int/news-room/fact-sheets/detail/deafness-and-hearing-loss},
        urlaccessdate = {16 mar. 2024},
    language = {en},
    urldate = {2023-12-21},
    author = {{World Health Organization}},
    year = {2023},
}

@misc{mp-pose,
    title = {Pose landmark detection guide},
    url = {https://developers.google.com/mediapipe/solutions/vision/pose_landmarker#:~:text=The%20MediaPipe%20Pose%20Landmarker%20task,with%20single%20images%20or%20video.},
        urlaccessdate = {23 mar. 2024},
    language = {en},
    author = {{MediaPipe Pose}},
    year = {2024},
}

% PNAD 2022 ("Censo" de Pessoas com Deficiência)
@misc{ibge_censo_2022,
    title = {Pessoas com deficiência têm menor acesso à educação, ao trabalho e à renda},
    url = {https://agenciadenoticias.ibge.gov.br/agencia-noticias/2012-agencia-de-noticias/noticias/37317-pessoas-com-deficiencia-tem-menor-acesso-a-educacao-ao-trabalho-e-a-renda},
  urlaccessdate = {16 mar. 2024},
    journal = {Agência de Notícias do IBGE},
    author = {Irene Gomes},
    year = {2023},
}

% Leis Brasileiras de LIBRAS
@misc{LeiBrasil_10436_2002,
  author        = {Brasil},
  title         = {Lei nº 10.436, de 24 de abril de 2002},
  journal       = {Diário Oficial [da República Federativa do Brasil]},
  year          = {2002},
  date          = {2002-04-24},
  organization  = {Brasil},
  url           = {https://www.planalto.gov.br/ccivil_03/leis/2002/l10436.htm},
  urlaccessdate = {16 mar. 2024},
  address       = {Dispõe sobre a {Língua} {Brasileira} de {Sinais} - {Libras} e dá outras providências. Brasília, DF},
  shorttitle    = {Lei nº 10.436/2002},
}

@misc{DecretoBrasil_5626_2005,
  author        = {Brasil},
  title         = {Decreto nº 5.626, de 22 de dezembro de 2005},
  journal       = {Diário Oficial [da República Federativa do Brasil]},
  year          = {2005},
  date          = {2005-12-22},
  organization  = {Brasil},
  url           = {https://www.planalto.gov.br/ccivil_03/_ato2004-2006/2005/decreto/d5626.htm},
  urlaccessdate = {16 mar. 2024},
  address       = {Regulamenta a {Lei} nº 10.436, de 24 de abril de 2002, que dispõe sobre a {Língua} {Brasileira} de {Sinais} - {Libras}, e o art. 18 da {Lei} nº 10.098, de 19 de dezembro de 2000. Brasília, DF},
  shorttitle    = {Decreto nº 5.626/2005},
}

@misc{LeiBrasil_13055_2014,
  author        = {Brasil},
  title         = {Lei nº 13.055, de 22 de dezembro de 2014},
  journal       = {Diário Oficial [da República Federativa do Brasil]},
  year          = {2014},
  date          = {2014-12-22},
  organization  = {Brasil},
  url           = {https://www.planalto.gov.br/ccivil_03/_ato2011-2014/2014/lei/l13055.htm},
  urlaccessdate = {16 mar. 2024},
  address       = {Institui o {Dia} {Nacional} da {Língua} {Brasileira} de {Sinais} - {LIBRAS} e dispõe sobre sua comemoração. Brasília, DF},
  shorttitle    = {Lei nº 13.055/2014},
}

% Dicionário de LIBRAS por Capovilla: 14.500 sinais em entradas lexicais individuais
@book{capovilla_dicionario-libras_2017,
    edition = {1},
    title = {Dicionário da {Língua} de {Sinais} do {Brasil}},
        subtitle = {{A} {Libras} em suas {Mãos}},
    isbn = {978-85-314-1645-3},
    publisher = {EDUSP},
    author = {Capovilla, Fernando César and Raphael, Walkiria Duarte and Martins, Antonielle Cantarelli and Temoteo, Janice Gonçalves},
    year = {2017},
}

@book{The-Process-and-Effects-of-Mass-Communication-Schramm-1960,
    edition = {4},
    title = {The Process and Effects of Mass Communication},
    isbn = {978-0252001970},
    publisher = {University of Illinois Press},
    author = {Schramm, Wilbur},
    year = {1960},
}

@misc{usp_dicionario-libras-divulga_2018,
    title = {“{Dicionário} da {Língua} de {Sinais}” exigiu 25 anos de pesquisas},
    url = {https://jornal.usp.br/cultura/dicionario-da-lingua-de-sinais-exigiu-25-anos-de-pesquisas/},
    abstract = {Lançada pela Editora da USP, obra recebeu prêmio da Associação Brasileira das Editoras Universitárias (Abeu)},
    language = {pt-BR},
    urldate = {2023-12-21},
        urlaccessdate = {16 mar. 2024},
    journal = {Jornal da USP},
    author = {Costa, Claudia},
    month = dec,
    year = {2018},
}

@INPROCEEDINGS{peres,
  author={Peres, Sarajane Marques and Flores, Franklin Cesar and Veronez, Denise and Olguin, Carlos Jose Maria},
  booktitle={2006 Ninth Brazilian Symposium on Neural Networks (SBRN'06)}, 
  title={LIBRAS Signals Recognition: a study with Learning Vector Quantization and Bit Signature}, 
  year={2006},
  volume={},
  number={},
  pages={119-124},
  keywords={Vector quantization;Natural languages;Artificial neural networks;Neurons;Pattern recognition;Computer science;Image recognition;Deafness;Neural networks;Supervised learning},
  doi={10.1109/SBRN.2006.26},
  abstract = {Learning Vector Quantization is a kind of artificial neural network with a competitive and supervised learning. This technique is commonly used to patterns recognition tasks. This artificial neural network was applied to the LIBRAS signals recognition problem, where the images representation was done with bits signatures. This is an unusual combination form and it seems promising, as inferred by the analysis of the results which are shown in this paper.},
  url = {https://ieeexplore.ieee.org/document/4026821},
    urlaccessdate = {23 mar. 2024}
}


@misc{/o-que-e-libras,
    title = {"O que é Libras?"},
    url = {https://www.libras.com.br/o-que-e-libras},
    abstract = {Libras é a sigla da Língua Brasileira de Sinais, uma língua de modalidade gestual-visual onde é possível se comunicar através de gestos, expressões faciais e corporais. É reconhecida como meio legal de comunicação e expressão desde 24 de Abril de 2002, através da Lei nº 10.436. A Libras é muito utilizada na comunicação com pessoas surdas, sendo, portanto, uma importante ferramenta de inclusão social.},
    language = {pt-BR},
    urldate = {2020-03-19},
        urlaccessdate = {16 mar. 2024},
    author = {Cristiano, Almir},
    month = may,
    year = {2017},
}

@misc{os-cinco-parametros-da-libras,
    title = {"Os Cinco Parâmetros da Libras"},
    url = {https://www.libras.com.br/os-cinco-parametros-da-libras},
    abstract = {As línguas que as comunidades surdas do mundo desenvolveram, passam por processos de denominação um pouco diferentes, embora comparáveis em alguns pontos aos das outras línguas orais. Quando falamos sobre os articuladores da língua de sinais, certamente podemos pensar em mãos. Mas na realidade, são usados como articuladores, além de mãos, outras partes do corpo, como a cabeça, face e tronco.},
    language = {pt-BR},
    urldate = {2018-08-26},
        urlaccessdate = {16 mar. 2024},
    author = {Cristiano, Almir},
    month = may,
    year = {2018},
}

@article{al-hammadi_hand_2020,
	title = {Hand {Gesture} {Recognition} for {Sign} {Language} {Using} {3DCNN}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.2990434},
	abstract = {Recently, automatic hand gesture recognition has gained increasing importance for two principal reasons: the growth of the deaf and hearing-impaired population, and the development of vision-based applications and touchless control on ubiquitous devices. As hand gesture recognition is at the core of sign language analysis a robust hand gesture recognition system should consider both spatial and temporal features. Unfortunately, finding discriminative spatiotemporal descriptors for a hand gesture sequence is not a trivial task. In this study, we proposed an efficient deep convolutional neural networks approach for hand gesture recognition. The proposed approach employed transfer learning to beat the scarcity of a large labeled hand gesture dataset. We evaluated it using three gesture datasets from color videos: 40, 23, and 10 classes were used from these datasets. The approach obtained recognition rates of 98.12\%, 100\%, and 76.67\% on the three datasets, respectively for the signer-dependent mode. For the signer-independent mode, it obtained recognition rates of 84.38\%, 34.9\%, and 70\% on the three datasets, respectively.},
	journal = {IEEE Access},
	author = {Al-Hammadi, Muneer and Muhammad, Ghulam and Abdul, Wadood and Alsulaiman, Mansour and Bencherif, Mohamed A. and Mekhtiche, Mohamed Amine},
	year = {2020},
	keywords = {3DCNN, Assistive technology, computer vision, Data preprocessing, deep learning, Deep learning, Feature extraction, Gesture recognition, hand gesture recognition, Human computer interaction, sign language recognition, Spatiotemporal phenomena, transfer learning},
	pages = {79491--79509},
}

@INPROCEEDINGS{survey,
  author={Kausar, Sumaira and Javed, M. Younus},
  booktitle={2011 Frontiers of Information Technology}, 
  title={A Survey on Sign Language Recognition}, 
  year={2011},
  volume={},
  number={},
  pages={95-98},
  keywords={Information technology;Sign Language;dynamic signs;static signs},
  doi={10.1109/FIT.2011.25},
    abstract = {Sign Language (SL) recognition is getting more and more attention of the researchers due to its widespread applicability in many fields. This paper is based on the survey of the current research trends in the field of SL recognition to highlight the current status of different research aspects of the area. Paper also critically analyzed the current research to identify the problem areas and challenges faced by the researchers. This identification is aimed at providing guideline for the future advances in the field.}
}

@article{assamese-sl,
title = {Real-time Assamese Sign Language Recognition using MediaPipe and Deep Learning},
journal = {Procedia Computer Science},
volume = {218},
pages = {1384-1393},
year = {2023},
note = {International Conference on Machine Learning and Data Engineering},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.01.117},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923001175},
urlaccessdate = {16 mar. 2024},
author = {Jyotishman Bora and Saine Dehingia and Abhijit Boruah and Anuraag Anuj Chetia and Dikhit Gogoi},
keywords = {Sign Language Recognition, Assamese Sign Language, Gesture recognition, 3D Image Recognition, MediaPipe, Neural network},
abstract = {People lacking the sense of hearing and the ability to speak have undeniable communication problems in their life. People with hearing and speech problems communicate using sign language with themselves and others. Sign language is not essentially known to a more significant portion of the human population who uses spoken and written language for communication. Therefore, it is a necessity to develop technological tools for interpretation of sign language. Much research have been carried out to acknowledge sign language using technology for most global languages. But there are still scopes of development of tools and techniques for sign language development for local dialects. There are 22 modern Indian languages and more than 19000 languages that are spoken regionally as mother tongue. This work attempts to develop a technical approach for recognizing Assamese Sign Language, which is one of the 22 modern languages of India. Using machine learning techniques, this work tried to establish a system for identifying the hand gestures from Assamese Sign Language. A combination of two-dimensional and three-dimensional images of Assamese gestures has been used to prepare a dataset. The MediaPipe framework has been implemented to detect landmarks in the images. An Assamese Sign Language dataset of 2094 data points has been generated, including nine static gestures with vowels and consonants (অ, ই, ঈ, উ, এ, ও, ক, জ, ল) from the Assamese Sign Language. The dataset was used for training of a feed-forward neural network. The model yielded an accuracy of 99%. The results reveal that the method implemented in this work is effective for the recognition of the other alphabets and gestures in the Assamese Sign Language. This method could also be tried and tested for the recognition of signs and gestures for various other local languages of India.}
}

@article{american-sl,
title = {American Sign Language Recognition for Alphabets Using MediaPipe and LSTM},
journal = {Procedia Computer Science},
volume = {215},
pages = {642-651},
year = {2022},
note = {4th International Conference on Innovative Data Communication Technology and Application},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.066},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922021378},
urlaccessdate = {16 mar. 2024},
author = {B Sundar and T Bagyammal},
keywords = {Hand Gesture Recognition, American Sign Language (ASL), Mediapipe, Long Short Term Memory (LSTM), Human-Computer Interface},
abstract = {With the advancement of today's technologies in artificial intelligence, humans tend to use hand gestures in their communication to convey their ideas. Gesture recognition is an active area of research in the human-computer interface (HCI). Gesture recognition is important for communication between deaf-mute people, HCI, robot control, home automation, and medical applications. In this article, a simple and efficient vision-based approach for American Sign Language (ASL) alphabets recognition has been discussed to recognize both static and dynamic gestures. Mediapipe introduced by Google had been used to get hand landmarks and a custom dataset has been created and used for the experimental study. Hand gesture recognition has been done by using Long short-term memory (LSTM). The proposed system has been investigated with 26 alphabets and an accuracy of 99% has been achieved. This work can be used to convert hand gestures into text.}
}

@techreport{zhang_mediapipe_2020,
	title = {{MediaPipe} {Hands}: {On}-device {Real}-time {Hand} {Tracking}},
	shorttitle = {{MediaPipe} {Hands}},
	url = {http://arxiv.org/abs/2006.10214},
	abstract = {We present a real-time on-device hand tracking pipeline that predicts hand skeleton from single RGB camera for AR/VR applications. The pipeline consists of two models: 1) a palm detector, 2) a hand landmark model. It's implemented via MediaPipe, a framework for building cross-platform ML solutions. The proposed model and pipeline architecture demonstrates real-time inference speed on mobile GPUs and high prediction quality. MediaPipe Hands is open sourced at https://mediapipe.dev.},
	number = {arXiv:2006.10214},
	urlaccessdate = {16 mar. 2024.},
	institution = {arXiv},
	author = {Zhang, Fan and Bazarevsky, Valentin and Vakunov, Andrey and Tkachenka, Andrei and Sung, George and Chang, Chuo-Ling and Grundmann, Matthias},
	month = jun,
	year = {2020},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 5 pages, 7 figures; CVPR Workshop on Computer Vision for Augmented and Virtual Reality, Seattle, WA, USA, 2020},
}

@article{anonimous-sl,
title = {Evaluation of Anonymized Sign Language Videos Filtered Using MediaPipe},
journal = {The Journal On Technology and Persons with Disabilites},
volume = {},
pages = {},
year = {2023},
issn = {2330-4219},
doi = {},
url = {https://scholarworks.calstate.edu/concern/publications/765378195},
urlaccessdate = {16 mar. 2024},
author = { Luna, Andrew and  Waller, James and Kushalnagar, Raja and Vogler, Christian},
keywords = {Real-time communication, MediaPipe, Privacy, Sign language},
abstract = {This study investigates the feasibility of using MediaPipe to anonymize sign language videos. Recent research has developed techniques for anonymizing the identity of a signer in a video, while preserving the signed message. Many of these prototypes are computationally intensive and are not currently useable for everyday automated real-time use. This gap MediaPipe, a tool developed by Google for tracking body movement in video, could be feasible for real-time anonymization, but has not yet been evaluated for its feasibility in sign anonymization. We fill this gap with a study in which deaf signers (n=10) view two filters developed using MediaPipe: a face mesh filter that covers only the face with an avatar-like face mask and a silhouette filter that covers the whole body in a solid monochrome, with interconnected dots showing the skeleton of the signer. Results show that signers are adept at understanding and reproducing short sentences covered by either filter. However, the filters are described as unnatural, and signers note facial movements are limited. We conclude that MediaPipe is likely robust enough for understanding manual information in signs but not necessarily for capturing facial information, and we suggest further improvements to the two filters.}
}

@Article{leapmotion-sl,
AUTHOR = {Chong, Teak-Wei and Lee, Boon-Giin},
TITLE = {American Sign Language Recognition Using Leap Motion Controller with Machine Learning Approach},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {10},
ARTICLE-NUMBER = {3554},
URL = {https://www.mdpi.com/1424-8220/18/10/3554},
urlaccessdate = {16 mar. 2024},
PubMedID = {30347776},
ISSN = {1424-8220},
ABSTRACT = {Sign language is intentionally designed to allow deaf and dumb communities to convey messages and to connect with society. Unfortunately, learning and practicing sign language is not common among society; hence, this study developed a sign language recognition prototype using the Leap Motion Controller (LMC). Many existing studies have proposed methods for incomplete sign language recognition, whereas this study aimed for full American Sign Language (ASL) recognition, which consists of 26 letters and 10 digits. Most of the ASL letters are static (no movement), but certain ASL letters are dynamic (they require certain movements). Thus, this study also aimed to extract features from finger and hand motions to differentiate between the static and dynamic gestures. The experimental results revealed that the sign language recognition rates for the 26 letters using a support vector machine (SVM) and a deep neural network (DNN) are 80.30% and 93.81%, respectively. Meanwhile, the recognition rates for a combination of 26 letters and 10 digits are slightly lower, approximately 72.79% for the SVM and 88.79% for the DNN. As a result, the sign language recognition system has great potential for reducing the gap between deaf and dumb communities and others. The proposed prototype could also serve as an interpreter for the deaf and dumb in everyday life in service sectors, such as at the bank or post office.},
DOI = {10.3390/s18103554}
}

@article{10.1093/iwc/iwac020,
    author = {Silva, André Luiz da Cunha and Sá, Tatiane Militão de and Diniz, Ruan Sousa and Ferreira, Simone Bacellar Leal and Siqueira, Sean Wolfgand Matsui and Bourguignon, Saulo Cabral},
    title = "{Prescriptive and Semantic Analysis of an Automatic Sign Language Translation: Cases on VLibras Avatar Translation Using Video Interviews and Textual Interactions With a Chatbot}",
    journal = {Interacting with Computers},
    volume = {35},
    number = {2},
    pages = {231-246},
    year = {2022},
    month = {10},
    abstract = "{Algorithms designed to translate textual content into sign language (SL) expressed through avatars have been used to reduce accessibility barriers. Our research aimed to identify whether the VLibras tool, widely adopted on Brazilian government websites, is an effective accessibility solution for automatic translation into SL. It is an exploratory and applied qualitative research project involving a bibliographic review and support from expert interpreters. We conducted two experimental studies using sequential chronological cuts and applying prescriptive and semantic analyses. We present evidence that there is no actual translation into SL in the automatic translation process performed by the VLibras translation algorithm (TA) but only a transposition of part of the SL lexicon to the Portuguese morphosyntactic structure. The automatic translation of long texts and texts with complex syntactic structures results in excessive pauses and dactylology for words that have a sign registered in the basic SL dictionary. Using human–computer interaction concepts to evaluate automatic translation into sign language by the VLibras TA expands the existing theoretical discussion. It also contributes to minimizing communication problems caused by the discrepancy between the original message and the machine translation, a practical applicability of this study.}",
    issn = {1873-7951},
    doi = {10.1093/iwc/iwac020},
    url = {https://doi.org/10.1093/iwc/iwac020},
    eprint = {https://academic.oup.com/iwc/article-pdf/35/2/231/51667308/iwac020.pdf},
    urlaccessdate = {17 mar. 2024}
}


@Article{avatarsin-sl,
AUTHOR = {Wolfe, Rosalee and McDonald, John C. and Hanke, Thomas and Ebling, Sarah and Van Landuyt, Davy and Picron, Frankie and Krausneker, Verena and Efthimiou, Eleni and Fotinea, Evita and Braffort, Annelies},
TITLE = {Sign Language Avatars: A Question of Representation},
JOURNAL = {Information},
VOLUME = {13},
YEAR = {2022},
NUMBER = {4},
ARTICLE-NUMBER = {206},
URL = {https://www.mdpi.com/2078-2489/13/4/206},
ISSN = {2078-2489},
ABSTRACT = {Given the achievements in automatically translating text from one language to another, one would expect to see similar advancements in translating between signed and spoken languages. However, progress in this effort has lagged in comparison. Typically, machine translation consists of processing text from one language to produce text in another. Because signed languages have no generally-accepted written form, translating spoken to signed language requires the additional step of displaying the language visually as animation through the use of a three-dimensional (3D) virtual human commonly known as an avatar. Researchers have been grappling with this problem for over twenty years, and it is still an open question. With the goal of developing a deeper understanding of the challenges posed by this question, this article gives a summary overview of the unique aspects of signed languages, briefly surveys the technology underlying avatars and performs an in-depth analysis of the features in a textual representation for avatar display. It concludes with a comparison of these features and makes observations about future research directions.},
DOI = {10.3390/info13040206},
urlaccessdate = {17 mar. 2024}
}

@inproceedings{mediapipe-to-unity,
author = {Zitong, Zhou and Yanhui, Lv},
title = {{The application of gesture recognition based on MediaPipe in virtual scene}},
volume = {12636},
booktitle = {Third International Conference on Machine Learning and Computer Application (ICMLCA 2022)},
editor = {Shuhong Ba and Fan Zhou},
organization = {International Society for Optics and Photonics},
publisher = {SPIE},
pages = {126361O},
keywords = {MediaPipe, Human-Computer Interaction, Virtual Reality},
year = {2023},
doi = {10.1117/12.2675148},
URL = {https://doi.org/10.1117/12.2675148},
urlaccessdate = {17 mar. 2024}
}

%fontes do sementille
@misc{bazarevsky2020blazepose,
      title={BlazePose: On-device Real-time Body Pose tracking}, 
      author={Valentin Bazarevsky and Ivan Grishchenko and Karthik Raveendran and Tyler Zhu and Fan Zhang and Matthias Grundmann},
      year={2020},
      eprint={2006.10204},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url = {https://arxiv.org/abs/2006.10204},
        urlaccessdate = {23 mar. 2024}
}

@INPROCEEDINGS{humanpose-estimation,
  author={Amrutha, K and Prabu, P and Paulose, Joy},
  booktitle={2021 Innovations in Power and Advanced Computing Technologies (i-PACT)}, 
  title={Human Body Pose Estimation and Applications}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  keywords={Technological innovation;Computational modeling;Pose estimation;Pipelines;Gesture recognition;Assistive technologies;Real-time systems;2D pose estimation;3D pose estimation;body modelling;sign language recognition;MediaPipe;Holistic Pipeline estimation},
  doi={10.1109/i-PACT52855.2021.9696513},
  url = {https://ieeexplore.ieee.org/document/9696513},
    urlaccessdate = {23 mar. 2024}
}

@INPROCEEDINGS{heterogeneous-avatar-animation,
  author={Song, Wenfeng and Zhang, Xinyu and Gao, Yang and Luo, Yifan and Wang, Haoxiang and Wang, Xianfei and Hou, Xia},
  booktitle={2023 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={UPSR: a Unified Proxy Skeleton Retargeting Method for Heterogeneous Avatar Animation}, 
  year={2023},
  volume={},
  number={},
  pages={867-868},
  keywords={Three-dimensional displays;Shape;Avatars;User interfaces;Animation;Skeleton;Motion capture;Computing methodologies-Computer graphics-Animation-Motion capture;},
  doi={10.1109/VRW58643.2023.00276},
  url = {https://ieeexplore.ieee.org/document/10108598},
    urlaccessdate = {23 mar. 2024}
}

@phdthesis{rodrigues_v-librasil_2021,
    type = {Dissertação de {Mestrado}},
    title = {V-{LIBRASIL} : uma base de dados com sinais na língua brasileira de sinais ({Libras})},
    url = {https://repositorio.ufpe.br/handle/123456789/43491},
        urlaccessdate = {23 mar. 2024},
    school = {Universidade Federal de Pernambuco},
    author = {Rodrigues, Ailton José},
    month = aug,
    year = {2021}
}

@phdthesis{landmarks-sarmento,
    type = {Dissertação de {Mestrado}},
    title = {Integração de Datasets de Vídeo para Tradução Automática da Libras com Aprendizado Profundo},
    url = {https://www.teses.usp.br/teses/disponiveis/55/55137/tde-10012024-093541/pt-br.php},
        urlaccessdate = {23 mar. 2024},
    school = {Universidade de São Paulo},
    author = {Sarmento, Amanda Hellen de Avellar},
    year = {2023}
}

@article{dias,
author = {Dias, Lívia and Mariani Braz, Ruth and Delou, Cristina and Winagraski, Erika and Carvalho, Helder and Castro, Helena},
year = {2014},
month = {01},
pages = {491-500},
title = {Deafness and the Educational Rights: A Brief Review through a Brazilian Perspective},
volume = {05},
journal = {Creative Education},
doi = {10.4236/ce.2014.57058},
url = {https://www.scirp.org/journal/paperinformation?paperid=45393},
    urlaccessdate = {24 mar. 2024}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Fundamentação Teórica %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@book{Libras-em-Contexto-Livro-Professor,
    author = {Felipe, Tanya Amara and Monteiro, Myrna},
    title = {Libras em Contexto: Curso Básico : Livro do Professor},
    publisher = {WalPrint},
    year = {2007},
    edition = {6},
    city = {Brasília},
    page = {448}
}

@misc{mediapipe-solutions,
    title = {MediaPipe Solutions Guide},
    url = {https://ai.google.dev/edge/mediapipe/solutions/guide},
        urlaccessdate = {22 out. 2024},
    language = {en},
    author = {{Google AI Edge}},
    year = {2024},
}

@book{c++-programming-language,
    author = {Stroustrup, Bjarne},
    title = {The C++ Programming Language},
    publisher = {Addison-Wesley},
    year = {2013},
    edition = {4},
    isbn = {978-0-321-56384-2},
    page = {1280}
}

@misc{mp-pose-github,
    title = {MediaPipe Pose},
    url = {https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/pose.md},
        urlaccessdate = {22 out. 2024},
    language = {en},
    author = {{Google AI Edge}},
    year = {2023},
}

@article{continuum-milgram,
    author = {Milgram, Paul and Takemura, Haruo and Utsumi, Akira and Kishino, Fumio},
    title = {Augmented reality: a class of displays on the reality-virtuality continuum},
    journal = {Telemanipulator and Telepresence Technologies},
    year = {1995},
    url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/2351/1/Augmented-reality--a-class-of-displays-on-the-reality/10.1117/12.197321.short},
    urlaccessdate = {22 out. 2024},
    doi = {10.1117/12.197321},
    publisher = {SPIE}
}

@article{virtual-reality-zheng,
    author = {Zheng, J. M and  Chan, K. W and Gibson, I},
    title = {Virtual reality},
    journal = {IEEE Potentials},
    year = {1998},
    url = {https://ieeexplore.ieee.org/abstract/document/666641/authors#authors},
    urlaccessdate = {22 out. 2024},
    doi = {10.1109/45.666641},
    publisher = {IEEE},
    volume = {17},
    number = {2},
    pages = {20-23},
}

@book{myths-realities-vr,
    author = {Arnaldi, Bruno and Guitton, Pascal and Moreau, Guillaume},
    title = {Virtual Reality and Augmented Reality: Myths and Realities},
    publisher = {Wiley-IEEE Press},
    year = {2018},
    edition = {1},
    isbn = {1-78630-105-9}
}

@INPROCEEDINGS{porfirio-dataset,
  author={Porfirio, Andres Jessé and Wiggers, Kelly Laís and Oliveira, Luiz E.S. and Weingaertner, Daniel},
  booktitle={2013 IEEE International Conference on Systems, Man, and Cybernetics}, 
  title={LIBRAS Sign Language Hand Configuration Recognition Based on 3D Meshes}, 
  year={2013},
  volume={},
  number={},
  pages={1588-1593},
  keywords={Three-dimensional displays;Feature extraction;Databases;Support vector machines;Assistive technology;Gesture recognition;Videos;sign language;hand configuration;3D mesh;LIBRAS},
  doi={10.1109/SMC.2013.274},
    url = {https://ieeexplore.ieee.org/document/6722027},
    urlaccessdate = {23 out. 2024}
}

@INPROCEEDINGS{gamiero-dataset,
  author={Gameiro, Priscila V. and Passos, Wesley L. and Araujo, Gabriel M. and de Lima, Amaro A. and Gois, Jonathan N. and Corbo, Anna R.},
  booktitle={2020 Latin American Robotics Symposium (LARS), 2020 Brazilian Symposium on Robotics (SBR) and 2020 Workshop on Robotics in Education (WRE)}, 
  title={A Brazilian Sign Language Video Database for Automatic Recognition}, 
  year={2020},
  volume={},
  number={},
  pages={1-6},
  keywords={Databases;Gesture recognition;Assistive technology;Video sequences;Radio frequency;Feature extraction;Task analysis;Sign Language;LIBRAS;Computer Vision;Gait Energy Image;Image Processing},
  doi={10.1109/LARS/SBR/WRE51543.2020.9307017},
    url = {https://ieeexplore.ieee.org/document/9307017},
    urlaccessdate = {23 out. 2024}
}

@article{CERNA2021114179,
title = {A multimodal LIBRAS-UFOP Brazilian sign language dataset of minimal pairs using a microsoft Kinect sensor},
journal = {Expert Systems with Applications},
volume = {167},
pages = {114179},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.114179},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420309143},
author = {Lourdes Ramirez Cerna and Edwin Escobedo Cardenas and Dayse Garcia Miranda and David Menotti and Guillermo Camara-Chavez},
keywords = {Sign language dataset, Minimal pairs, Sign language recognition, Dynamic images, RGB-D data, CNN},
abstract = {Sign language recognition has made significant advances in recent years. Many researchers show interest in encouraging the development of different applications to simplify the daily life of deaf people and to integrate them into the hearing society. The use of the Kinect sensor (developed by Microsoft) for sign language recognition is steadily increasing. However, there are limited publicly available RGB-D and skeleton joint datasets that provide complete information for dynamic signs captured by Kinect sensor, most of them lack effective and accurate labeling or stored in a single data format. Given the limitations of existing datasets, this article presents a challenging public dataset, named LIBRAS-UFOP. The dataset is based on the concept of minimal pairs, which follows specific categorization criteria; the signs are labeled correctly, and validated by an expert in sign language; the dataset presents complete RGB-D and skeleton data. The dataset consists of 56 different signs with high similarity grouped into four categories. Besides, a baseline method is presented that consists of the generation of dynamic images from each multimodal data, which are the input to two flow stream CNN architectures. Finally, we propose an experimental protocol to conduct evaluations on the proposed dataset. Due to the high similarity between signs, the experimental results using a baseline method reports a recognition rate of 74.25% on the proposed dataset. This result highlights how challenging this dataset is for sign language recognition and let room for future research works to improve the recognition rate.},
    urlaccessdate = {23 out. 2024}
}

@phdthesis{rezende-tese,
    author = {Tamires Martins Rezende},
    type = {Tese de Doutorado},
    title = {Reconhecimento automático de sinais da Libras : desenvolvimento da base de dados MINDS-Libras e modelos de redes convolucionais},
    school = {Universidade Federal de Minas Gerais},
    year = {2021},
    month = {jul.},
    urlaccessdate = {24 out. 2024},
    url = {http://hdl.handle.net/1843/39785}
}

@INPROCEEDINGS{sarmento-article,
  author={de Avellar Sarmento, Amanda Hellen and Antonelli Ponti, Moacir},
  booktitle={2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)}, 
  title={A Cross-Dataset Study on the Brazilian Sign Language Translation}, 
  year={2023},
  volume={},
  number={},
  pages={2808-2812},
  keywords={Training;Natural languages;Gesture recognition;Assistive technologies;Gaussian distribution;Skeleton;Random processes;LIBRAS Dataset;Sign Language Translation;Brazilian Sign Language},
  doi={10.1109/ICCVW60793.2023.00300},
    url ={https://ieeexplore.ieee.org/document/10350372},
    urlaccessdate = {24 out. 2024}
    }

@article{Nowak2018,
 title = {Avatars and computer-mediated communication: a review of the definitions, uses, and effects of digital representations},
 author = {Nowak, Kristine L. and Fox, Jesse},
 journal = {Review of Communication Research},
 pages = {30-53},
 volume = {6},
 year = {2018},
 issn = {2255-4165},
 doi = {10.12840/issn.2255-4165.2018.06.01.015},
 urn = {https://nbn-resolving.org/urn:nbn:de:0168-ssoar-55777-7},
 abstract = {Avatars are growing in popularity and present in many interfaces used for computer-mediated communication (CMC) including social media, e-commerce, and education. Communication researchers have been investigating avatars for over twenty years, and an examination of this literature reveals similarities but also notable discrepancies in conceptual definitions. The goal of this review is to provide a general overview of current debates, methodological approaches, and trends in findings. Our review synthesizes previous research in four areas. First, we examine how scholars have conceptualized the term “avatar,” identify similarities and differences across these definitions, and recommend that scholars use the term consistently. Next, we review theoretical perspectives relevant to avatar perception (e.g., the computers as social actors framework). Then, we examine avatar characteristics that communicators use to discern the humanity and social potential of an avatar (anthropomorphism, form realism, behavioral realism, and perceived agency) and discuss implications for attributions and communication outcomes. We also review findings on the social categorization of avatars, such as when people apply categories like sex, gender, race, and ethnicity to their evaluations of digital representations. Finally, we examine research on avatar selection and design relevant to communication outcomes. Here, we review both motivations in CMC contexts (such as self-presentation and identity expression) and potential effects (e.g., persuasion). We conclude with a discussion of future directions for avatar research and propose that communication researchers consider avatars not just as a topic of study, but also as a tool for testing theories and understanding critical elements of human communication. Avatar mediated environments provide researchers with a number of advantageous technological affordances that can enable manipulations that may be difficult or inadvisable to execute in natural environments. We conclude by discussing the use of avatar research to extend communication theory and our understanding of communication processes.},
 keywords = {computervermittelte Kommunikation; computer-mediated communication; Digitale Medien; digital media; Computerspiel; computer game; Repräsentation; representation; Selbstdarstellung; self-presentation; Identität; identity; virtuelle Realität; virtual reality; Kommunikationsverhalten; communication behavior},
    url = {https://nbn-resolving.org/urn:nbn:de:0168-ssoar-55777-7},
    urlaccessdate = {25 out. 2024}
    }

@book{stephenson2022snow,
  title={Snow Crash},
  author={Stephenson, Neal},
  isbn={978-0553088533},
  year={1992},
  edition = {1},
  publisher={Bantam Books},
  address = {Nova York}
}

@article{wang2024survey3dhumanavatar,
 title = {A Survey on 3D Human Avatar Modeling -- From Reconstruction to Generation},
 author = {Ruihe Wang and Yukang Cao and Kai Han and Kwan-Yee K. Wong},
 journal = {},
 pages = {},
 volume = {},
 year = {2024},
 doi = {2406.04253},
    url = {https://arxiv.org/abs/2406.04253},
    urlaccessdate = {25 out. 2024}
    }

@misc{unity_docs_2022,
	title = {Unity - {Manual}: {Unity} {User} {Manual} 2022.3 ({LTS})},
	url = {https://docs.unity3d.com/2022.3/Documentation/Manual/index.html},
        urlaccessdate = {25 out. 2024},
	language = {en},
	author = {{Unity Technologies}},
	year = {2022}
}

@article{skeleton-aware-retargeting,
author = {Aberman, Kfir and Li, Peizhuo and Lischinski, Dani and Sorkine-Hornung, Olga and Cohen-Or, Daniel and Chen, Baoquan},
title = {Skeleton-aware networks for deep motion retargeting},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3386569.3392462},
doi = {10.1145/3386569.3392462},
abstract = {We introduce a novel deep learning framework for data-driven motion retargeting between skeletons, which may have different structure, yet corresponding to homeomorphic graphs. Importantly, our approach learns how to retarget without requiring any explicit pairing between the motions in the training set.We leverage the fact that different homeomorphic skeletons may be reduced to a common primal skeleton by a sequence of edge merging operations, which we refer to as skeletal pooling. Thus, our main technical contribution is the introduction of novel differentiable convolution, pooling, and unpooling operators. These operators are skeleton-aware, meaning that they explicitly account for the skeleton's hierarchical structure and joint adjacency, and together they serve to transform the original motion into a collection of deep temporal features associated with the joints of the primal skeleton. In other words, our operators form the building blocks of a new deep motion processing framework that embeds the motion into a common latent space, shared by a collection of homeomorphic skeletons. Thus, retargeting can be achieved simply by encoding to, and decoding from this latent space.Our experiments show the effectiveness of our framework for motion retargeting, as well as motion processing in general, compared to existing approaches. Our approach is also quantitatively evaluated on a synthetic dataset that contains pairs of motions applied to different skeletons. To the best of our knowledge, our method is the first to perform retargeting between skeletons with differently sampled kinematic chains, without any paired examples.},
journal = {ACM Trans. Graph.},
month = aug,
articleno = {62},
numpages = {14},
keywords = {motion retargeting, neural motion processing},
urlaccessdate = {26 out. 2024},
}

@inproceedings{artigo-gercia-transfermotion,
author = {Poulios, Ilias and Pistola, Theodora and Symeonidis, Spyridon and Diplaris, Sotiris and Ioannidis, Konstantinos and Vrochidis, Stefanos and Kompatsiaris, Ioannis},
title = {Enhanced real-time motion transfer to 3D avatars using RGB-based human 3D pose estimation},
year = {2024},
isbn = {9798400717949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3672406.3672427},
doi = {10.1145/3672406.3672427},
abstract = {Human motion transfer on 3D avatars has witnessed substantial progress, driven by the advancements of 3D pose estimation using RGB data. This technology analyzes human movements captured through RGB cameras, enabling tracking of 3D body landmarks and leading to the animation of 3D avatars. Utilizing RGB input offers a range of advantages, democratizing avatar creation by eliminating the need for specialized equipment, such as sensors, markers, or specialized studios. Recent years have seen remarkable strides in this field, leveraging deep learning models and sophisticated computer vision algorithms to capture intricate movements and gestures from RGB video footage. This study introduces a novel real-time approach leveraging RGB input to generate realistic 3D animations. It comprises three phases: i) 3D human pose estimation using MediaPipe, ii) correction of MediaPipe’s landmarks’ inaccuracies, especially regarding depth dimension, and incorporation of bones’ rotation information, and, finally, iii) transfer of the motion to the target 3D avatar.},
booktitle = {Proceedings of the 2024 ACM International Conference on Interactive Media Experiences Workshops},
pages = {88–99},
numpages = {12},
keywords = {3D Pose Estimation, Avatar Animation, Computer Vision, Deep Learning, MediaPipe},
location = {Stockholm, Sweden},
series = {IMXw '24},
urlaccessdate = {26 out. 2024}
}